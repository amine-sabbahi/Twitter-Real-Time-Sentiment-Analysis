{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b2bff9-1683-4d86-bf30-4984fc3391b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, regexp_replace, lower, concat_ws\n",
    "from pyspark.sql.functions import  expr, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from pyspark.ml.feature import  StringIndexer, CountVectorizer, IDF\n",
    "from pyspark.ml.classification import LogisticRegression,LinearSVC,OneVsRest, DecisionTreeClassifier, RandomForestClassifier, NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7976cd17-8209-4f53-be12-e0b098bf9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Twitter2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47b8fd7-7693-40d4-a294-183ebfb31e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.56.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Twitter2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x224fab87810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dce125-d3f3-4295-b918-c0b57f79e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le schéma (noms et types de colonnes)\n",
    "schema = StructType([\n",
    "    StructField(\"Tweet ID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Tweet content\", StringType(), True),\n",
    "])\n",
    "# Charger les données à partir du fichier CSV sans en-tête, avec le schéma défini\n",
    "sentiment_data = spark.read.csv('twitter_training.csv', header=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a91d9d-de00-47e2-aa75-edaaf419de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Tweet ID: integer (nullable = true)\n",
      " |-- Entity: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      " |-- Tweet content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "3cfa0101-7a4f-4562-9e67-49d54b364ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+\n",
      "|Tweet ID|     Entity|Sentiment|       Tweet content|\n",
      "+--------+-----------+---------+--------------------+\n",
      "|    2401|Borderlands| Positive|im getting on bor...|\n",
      "|    2401|Borderlands| Positive|I am coming to th...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|\n",
      "|    2402|Borderlands| Positive|                 was|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Vita...|\n",
      "|    2403|Borderlands|  Neutral|Live Rock - Hard ...|\n",
      "|    2403|Borderlands|  Neutral|I-Hard like me, R...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|\n",
      "+--------+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les premières lignes de votre DataFrame\n",
    "sentiment_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292f4bee-fc5f-451e-bc48-d8e7a34c18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = sentiment_data.drop(\"Tweet ID\")\n",
    "sentiment_data = sentiment_data.drop(\"Entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8ac333a7-40cc-456f-b951-bcb299b83381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|Sentiment|       Tweet content|\n",
      "+---------+--------------------+\n",
      "| Positive|im getting on bor...|\n",
      "| Positive|I am coming to th...|\n",
      "| Positive|im getting on bor...|\n",
      "| Positive|im coming on bord...|\n",
      "| Positive|im getting on bor...|\n",
      "| Positive|im getting into b...|\n",
      "| Positive|So I spent a few ...|\n",
      "| Positive|So I spent a coup...|\n",
      "| Positive|So I spent a few ...|\n",
      "| Positive|So I spent a few ...|\n",
      "| Positive|2010 So I spent a...|\n",
      "| Positive|                 was|\n",
      "|  Neutral|Rock-Hard La Varl...|\n",
      "|  Neutral|Rock-Hard La Varl...|\n",
      "|  Neutral|Rock-Hard La Varl...|\n",
      "|  Neutral|Rock-Hard La Vita...|\n",
      "|  Neutral|Live Rock - Hard ...|\n",
      "|  Neutral|I-Hard like me, R...|\n",
      "| Positive|that was the firs...|\n",
      "| Positive|this was the firs...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a59dcf19-181b-493c-9ac7-125181f49f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74682"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed53eec6-af8f-43d5-805c-6025a7fd02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = sentiment_data.dropna(subset=[\"Tweet content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "1ffdc761-7fe2-4d56-8d5d-74803270a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73824"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adae6f0f-9e0c-4cc5-8594-4e1c5989550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer le texte pour supprimer les caractères non alphabétiques et \n",
    "sentiment_data = sentiment_data.withColumn('Cleaned Tweet content', regexp_replace(col('Tweet content'), r'[^a-zA-Z\\s]', ''))\n",
    "sentiment_data = sentiment_data.withColumn('Cleaned Tweet content', lower(col('Cleaned Tweet content')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dcba3f8-3993-43ea-a299-5f4070adcf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  filtrer les lignes où le tweet nettoyé est vide\n",
    "sentiment_data = sentiment_data.filter(sentiment_data['Cleaned Tweet content'] != \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "77a173eb-50a9-4efe-8237-e814128f17b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73802"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "252e7a01-c3b1-40ce-9ede-48da8a7d9f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+\n",
      "|Sentiment|       Tweet content|Cleaned Tweet content|\n",
      "+---------+--------------------+---------------------+\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|I am coming to th...| i am coming to th...|\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|im coming on bord...| im coming on bord...|\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|im getting into b...| im getting into b...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|So I spent a coup...| so i spent a coup...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|2010 So I spent a...|  so i spent a few...|\n",
      "| Positive|                 was|                  was|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Vita...| rockhard la vita ...|\n",
      "|  Neutral|Live Rock - Hard ...| live rock  hard m...|\n",
      "|  Neutral|I-Hard like me, R...| ihard like me rar...|\n",
      "| Positive|that was the firs...| that was the firs...|\n",
      "| Positive|this was the firs...| this was the firs...|\n",
      "+---------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "db652434-c308-4e32-b8a4-852e23e59d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+\n",
      "|Sentiment|       Tweet content|Cleaned Tweet content|\n",
      "+---------+--------------------+---------------------+\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|I am coming to th...| i am coming to th...|\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|im coming on bord...| im coming on bord...|\n",
      "| Positive|im getting on bor...| im getting on bor...|\n",
      "| Positive|im getting into b...| im getting into b...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|So I spent a coup...| so i spent a coup...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|\n",
      "| Positive|2010 So I spent a...|  so i spent a few...|\n",
      "| Positive|                 was|                  was|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|\n",
      "|  Neutral|Rock-Hard La Vita...| rockhard la vita ...|\n",
      "|  Neutral|Live Rock - Hard ...| live rock  hard m...|\n",
      "|  Neutral|I-Hard like me, R...| ihard like me rar...|\n",
      "| Positive|that was the firs...| that was the firs...|\n",
      "| Positive|this was the firs...| this was the firs...|\n",
      "+---------+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28584f2-b377-4ef9-b93b-e4e90cf5b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les étiquettes de sentiment (sentiment labels) en indices numériques\n",
    "sentiment_data = sentiment_data.withColumn('label', when(col('Sentiment')=='Positive', 1)\n",
    "                .when(col('Sentiment')=='Negative',2)\n",
    "              .when(col('Sentiment')=='Neutral',0).when(col('Sentiment')=='Irrelevant',3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "e33a0739-f503-4c86-b229-562aca09d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+-----+\n",
      "|Sentiment|       Tweet content|Cleaned Tweet content|label|\n",
      "+---------+--------------------+---------------------+-----+\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|I am coming to th...| i am coming to th...|    1|\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|im coming on bord...| im coming on bord...|    1|\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|im getting into b...| im getting into b...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|So I spent a coup...| so i spent a coup...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|2010 So I spent a...|  so i spent a few...|    1|\n",
      "| Positive|                 was|                  was|    1|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Vita...| rockhard la vita ...|    0|\n",
      "|  Neutral|Live Rock - Hard ...| live rock  hard m...|    0|\n",
      "|  Neutral|I-Hard like me, R...| ihard like me rar...|    0|\n",
      "| Positive|that was the firs...| that was the firs...|    1|\n",
      "| Positive|this was the firs...| this was the firs...|    1|\n",
      "+---------+--------------------+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08dc1968-8d39-484b-80dc-c364c4dd51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Diviser le texte en tokens (mots)\n",
    "tokenizer = Tokenizer(inputCol='Cleaned Tweet content', outputCol='words')\n",
    "# Suppression des stop words (mots vides)\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "# Créer un CountVectorizer\n",
    "count_vectorizer = CountVectorizer(inputCol='filtered_words', outputCol='raw_features')\n",
    "# Créer un IDF\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "647f000a-e219-47d2-b20c-bfc847cd54d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+-----+\n",
      "|Sentiment|       Tweet content|Cleaned Tweet content|label|\n",
      "+---------+--------------------+---------------------+-----+\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|I am coming to th...| i am coming to th...|    1|\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|im coming on bord...| im coming on bord...|    1|\n",
      "| Positive|im getting on bor...| im getting on bor...|    1|\n",
      "| Positive|im getting into b...| im getting into b...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|So I spent a coup...| so i spent a coup...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|So I spent a few ...| so i spent a few ...|    1|\n",
      "| Positive|2010 So I spent a...|  so i spent a few...|    1|\n",
      "| Positive|                 was|                  was|    1|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Varl...| rockhard la varlo...|    0|\n",
      "|  Neutral|Rock-Hard La Vita...| rockhard la vita ...|    0|\n",
      "|  Neutral|Live Rock - Hard ...| live rock  hard m...|    0|\n",
      "|  Neutral|I-Hard like me, R...| ihard like me rar...|    0|\n",
      "| Positive|that was the firs...| that was the firs...|    1|\n",
      "| Positive|this was the firs...| this was the firs...|    1|\n",
      "+---------+--------------------+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90bbfdd3-a53a-4a25-a7d7-aace42936572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de test (80% pour l'entraînement et 20% pour le test)\n",
    "train_data, test_data = sentiment_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "09159685-e042-47b6-93f3-e5228828b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8182005070924416\n"
     ]
    }
   ],
   "source": [
    "# Modèle de régression logistique\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_pipeline = Pipeline(stages=[tokenizer, remover, count_vectorizer, idf, lr])\n",
    "# Entraîner le modèle de régression logistique\n",
    "lr_model = lr_pipeline.fit(train_data)\n",
    "\n",
    "# Prédictions et évaluation sur l'ensemble de test\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aaec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.tosave(\"lr_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2f37b3-d61b-461b-a186-0b5207d1cfa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3878.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1824.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1824.0 (TID 5409) (192.168.56.1 executor driver): java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:153)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m svm_accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(svm_predictions)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvm_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mevaluate(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3878.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1824.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1824.0 (TID 5409) (192.168.56.1 executor driver): java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.immutable.List.foreach(List.scala:333)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy$lzycompute(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.accuracy(MulticlassMetrics.scala:188)\r\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:153)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Software caused connection abort: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flush(Unknown Source)\r\n\tat java.io.DataOutputStream.flush(Unknown Source)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:454)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "# Define the LinearSVC model\n",
    "svm = LinearSVC(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Define the OneVsRest classifier\n",
    "one_vs_rest = OneVsRest(classifier=svm, featuresCol='features', labelCol='label', predictionCol='svm_prediction')\n",
    "\n",
    "# Create the pipeline with necessary stages\n",
    "svm_pipeline = Pipeline(stages=[\n",
    "    tokenizer,  # Tokenizer stage\n",
    "    remover,  # StopWordsRemover stage\n",
    "    count_vectorizer,  # CountVectorizer stage\n",
    "    idf,  # IDF stage\n",
    "    one_vs_rest  # OneVsRest stage with LinearSVC\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "svm_model = svm_pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "svm_predictions = svm_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='svm_prediction', metricName='accuracy')\n",
    "svm_accuracy = evaluator.evaluate(svm_predictions)\n",
    "print(f\"Model accuracy: {svm_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8207759-3f23-4069-b1ec-85a8f73b8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.3482044956140351\n"
     ]
    }
   ],
   "source": [
    "# Pipeline pour Decision Tree\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "dt_pipeline = Pipeline(stages=[tokenizer, remover, count_vectorizer, idf, dt])\n",
    "\n",
    "# Entraîner le modèle d'arbre de décision\n",
    "dt_model = dt_pipeline.fit(train_data)\n",
    "\n",
    "# Prédictions et évaluation sur l'ensemble de test\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "dt_accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c06a65bf-b68e-438d-aabd-aec04fe400d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest model: 0.41248629385964913\n"
     ]
    }
   ],
   "source": [
    "# Configurer RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', maxDepth=10)\n",
    "\n",
    "# Construire le pipeline RandomForest\n",
    "rf_pipeline = Pipeline(stages=[tokenizer, remover, count_vectorizer, idf, rf])\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f'Accuracy of Random Forest model: {rf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6941afc0-7d57-4870-b986-a533c093e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes model: 0.7972176535087719\n"
     ]
    }
   ],
   "source": [
    "# Configurer NaiveBayes\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', smoothing=1.0, modelType='multinomial')\n",
    "\n",
    "# Construire le pipeline NaiveBayes\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, remover, count_vectorizer, idf, nb])\n",
    "\n",
    "# Entraîner le modèle sur les données d'entraînement\n",
    "nb_model = nb_pipeline.fit(train_data)\n",
    "\n",
    "# Faire des prédictions sur les données de test\n",
    "nb_predictions = nb_model.transform(test_data)\n",
    "\n",
    "# Évaluer le modèle sur les données de test\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(f'Accuracy of Naive Bayes model: {nb_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4f2d8a42-c12e-4543-9b9b-63ce46acfc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
